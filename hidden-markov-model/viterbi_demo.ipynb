{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of usage of the Hidden Markov Model code\n",
    "\n",
    "The hidden_markov_model class requires NumPy, but only NumPy. The other packages we use are for data loading and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are trying to solve is to infer from a given set of observation of die throws, which die are fair and which are loaded. We will use the Viterbi algorithm to solve this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our initial guesses for the initial probability of each state at a timestep t, the transition probabilities, and the emission probabilities. We initialize our states to 0 and 1 because there are two possible states at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prob = np.array([.5, .5])\n",
    "transition_prob = np.array(([0.95, .05], [.05, .95]))\n",
    "emission_prob = np.array(([1/6, 1/6, 1/6, 1/6, 1/6, 1/6], [1/10, 1/10, 1/10, 1/10, 1/10, 1/2]))\n",
    "states = np.array([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('~/Documents/machine-learning/data/hw11pb1.csv', header=None)\n",
    "data2 = pd.read_csv('~/Documents/machine-learning/data/hw11pb2.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our Pandas DataFrames to NumPy arrays and flatten them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.values.flatten()\n",
    "data2 = data2.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Hidden Markov Model class. \n",
    "\n",
    "It takes as arguments some starting probabilities, transition probabilities, emission probabilities, the set of possible states, and the observation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_markov_model:\n",
    "    def __init__(self, start_probs, transition_probs, emission_probs, state_set, observations):\n",
    "        self.start = start_probs\n",
    "        self.transition = transition_probs\n",
    "        self.emission = emission_probs\n",
    "        self.states = state_set\n",
    "        self.obs = observations\n",
    "        self.n_states = len(self.states)\n",
    "        self.t_obs = len(observations)\n",
    "        self.alpha = np.zeros((self.t_obs, self.n_states))\n",
    "        self.beta = np.zeros((self.t_obs, self.n_states))\n",
    "        self.underflow = np.ones(len(self.obs))\n",
    "    \n",
    "    def get_gammas(self, alphas, betas, T, N):\n",
    "        gammas = np.zeros((len(self.obs)-1, self.n_states))\n",
    "        xs = np.zeros((len(self.obs)-1, self.n_states, self.n_states))\n",
    "        for t in range(self.t_obs-1):\n",
    "            xinum = np.array([self.alpha[t]]*2).T * self.transition * np.array([self.beta[t+1]]*2)*np.array([self.emission[:, self.obs[t]-1]]*2)\n",
    "            xs[t] = xinum/np.sum(xinum)\n",
    "            gammas[t] = np.sum(xs[t], axis=1)\n",
    "        \n",
    "        return gammas, xs\n",
    "        \n",
    "    def EM(self, N, M, T, pi, A, B):\n",
    "        alphas = self.forward()\n",
    "        betas = self.backward()\n",
    "        \n",
    "        gammas, xs = self.get_gammas(alphas, betas, T, N)\n",
    "        \n",
    "        new_pi = gammas[0, :]\n",
    "        \n",
    "        new_a = np.sum(xs, axis=0)/(np.array([np.sum(gammas, axis=0)]*2).T)\n",
    "       \n",
    "        xinds = np.zeros((len(self.obs), len(self.emission[0])))\n",
    "        xinds[np.arange(len(self.obs)), self.obs-1] = 1\n",
    "        \n",
    "        new_b = (gammas.T@xinds[:-1])/np.array([np.sum(gammas, axis=0)]*6).T\n",
    "        \n",
    "        return new_pi, new_a, new_b\n",
    "        \n",
    "    def baumwelch(self, tolerance, maxiter):\n",
    "        T = self.t_obs\n",
    "        N = self.n_states\n",
    "        M = len(self.states)\n",
    "        likely = []\n",
    "        \n",
    "        pi = np.log(self.start)\n",
    "        A = np.log(self.transition)\n",
    "        B = np.log(self.emission)\n",
    "        \n",
    "        iter_ = 0\n",
    "        err = np.inf\n",
    "        while iter_ < maxiter or err > tolerance:\n",
    "            iter_ += 1\n",
    "            if iter_ % 100 == 0:\n",
    "                print('Iter:', iter_)\n",
    "            new_pi, new_a, new_b = self.EM(N, M, T, pi, A, B)\n",
    "            \n",
    "            err = np.abs(np.linalg.norm(A - new_a) + np.linalg.norm(B - new_b))/2\n",
    "            \n",
    "            pi, A, B = new_pi, new_a, new_b\n",
    "            \n",
    "        return pi, A, B\n",
    "    \n",
    "    def forward(self):\n",
    "        self.alpha[0] = self.start * self.emission[:, self.obs[0]-1]\n",
    "        for t in range(1, self.t_obs):\n",
    "            for k in self.states:\n",
    "                self.alpha[t, k] = (self.alpha[t-1]@self.transition[k]) * self.emission[k, self.obs[t]-1]\n",
    "            self.underflow[t] = np.sum(self.alpha[t])\n",
    "            self.alpha[t] = self.alpha[t]/self.underflow[t]\n",
    "        return self.alpha\n",
    "    \n",
    "    def backward(self):\n",
    "        self.beta[:, -1:] = 1\n",
    "        \n",
    "        for t in range(2, len(self.obs)+1):\n",
    "            for k in self.states:\n",
    "                self.beta[-t, k] = np.sum(self.beta[-t+1] * self.transition[k] * self.emission[:, self.obs[-t+1]-1])\n",
    "            self.beta[-t] = self.beta[-t]/self.underflow[-t]\n",
    "        return self.beta\n",
    "    \n",
    "    def viterbi(self):\n",
    "        N = self.transition.shape[0]\n",
    "        delta = np.zeros((self.t_obs, N))\n",
    "        psi = np.zeros((self.t_obs, N))\n",
    "        delta[0] = self.start * self.emission[:, self.obs[0]-1]\n",
    "        for tt in range(1, self.t_obs):\n",
    "            for j in range(N):\n",
    "                delta[tt, j] = np.max(delta[tt-1] * self.transition[:, j] * self.emission[j, self.obs[tt]-1])\n",
    "                psi[tt, j] = np.argmax(delta[tt-1] * self.transition[:, j])\n",
    "                \n",
    "        states = np.zeros(self.t_obs)\n",
    "        states[self.t_obs-1] = np.argmax(np.exp(delta[self.n_states-1]))\n",
    "        states = states.astype('int')\n",
    "        for t in range(self.t_obs-2, -1, -1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "        return states\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of the Hidden Markov Model class with the probability estimates created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm1 = hidden_markov_model(init_prob, transition_prob, emission_prob, states, data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here using the Viterbi algorithm to solve for which die are fair and which are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F',\n",
       "       'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L',\n",
       "       'F', 'F'], dtype='<U1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess = hmm1.viterbi()\n",
    "empt = np.empty(guess.shape, dtype='str')\n",
    "f_inds = np.where(guess == 0)\n",
    "l_inds = np.where(guess==1)\n",
    "empt[f_inds] = 'F'\n",
    "empt[l_inds] = 'L'\n",
    "empt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are solving a second problem using a second dataset. This dataset contains 1000 dice rolls, so we have much more information about the state of the die than before. We are going to use this increased information to solve for the model parameters using the Baum-Welch algorithm.\n",
    "\n",
    "In cell below, initializing the model with a guess of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.95, 0.05], [0.05, 0.95]])\n",
    "B = np.array(([[1/6, 1/6, 1/6, 1/6, 1/6, 1/6], [.1, .1, .1, .1, .1, .5]]))\n",
    "pi = np.array([1/2, 1/2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a hidden_markov_model object with the initial probability guesses above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm2 = hidden_markov_model(pi, A, B, states, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving for the true model parameters using Baum-Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100\n"
     ]
    }
   ],
   "source": [
    "solved_pi, solved_A, solve_B = hmm2.baumwelch(1e-2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi found through Baum Welch: [0.42671712 0.57328288]\n",
      "Transition probabilities found through Baum Welch: [[0.99065118 0.00934882]\n",
      " [0.16993597 0.83006403]]\n",
      "Emission probabilities found through Baum Welch: [[0.20145444 0.20746921 0.19321639 0.2003796  0.1272622  0.07021816]\n",
      " [0.10984433 0.10203983 0.1023266  0.10441057 0.06631893 0.51505975]]\n"
     ]
    }
   ],
   "source": [
    "print('Pi found through Baum Welch:', solved_pi)\n",
    "print('Transition probabilities found through Baum Welch:', solved_A)\n",
    "print('Emission probabilities found through Baum Welch:', solve_B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
